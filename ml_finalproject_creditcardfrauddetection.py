# -*- coding: utf-8 -*-
"""ML_FinalProject_CreditCardFraudDetection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gFZErqyXNC6F77siPh2hBfGwbBPwCCdK
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""**Main objective here would be to minimize the false negatives in the credit card dataset**"""

import pandas as pd
pd.set_option('display.max_columns', None)
df = pd.read_csv('/content/gdrive/My Drive/ML/creditcard.csv')
df.head()

# Describe the data (including means and standard deviations)
print("\nData description:")
print(df.describe())

# Check for missing values
print("Missing values summary:")
print(df.isnull().sum())

# Check the class distribution (number of frauds vs. normal transactions)
fraud_count = len(df[df["Class"] == 1])
normal_count = len(df[df["Class"] == 0])
print(f"\nClass Distribution:")
print(f"  - Frauds: {fraud_count} ({fraud_count / len(df) * 100:.2f}%)")
print(f"  - Normal Transactions: {normal_count} ({normal_count / len(df) * 100:.2f}%)")

import matplotlib.pyplot as plt

# Visualize class distribution with a bar chart
plt.bar(["Frauds", "Normal Transactions"], [fraud_count, normal_count])
plt.title("Class Distribution in Credit Card Transactions")
plt.xlabel("Transaction Class")
plt.ylabel("Number of Transactions")
plt.show()

# Descriptive statistics (grouped by class)
fraud_stats = df[df["Class"] == 1].describe().transpose()  # Focus on fraud class
normal_stats = df[df["Class"] == 0].describe().transpose()  # Focus on normal class

# Print descriptive statistics side-by-side
print("Descriptive Statistics (Fraud vs. Normal)")
print(pd.concat([fraud_stats, normal_stats], axis=1))

#Grouped comparison (example: t-test for transaction amount)
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns

amount_fraud = df[df["Class"] == 1]["Amount"]
amount_normal = df[df["Class"] == 0]["Amount"]
t_stat, p_value = ttest_ind(amount_fraud, amount_normal)

print(f"\nT-test (Amount): t={t_stat:.4f}, p-value={p_value:.4f}")  # Check for significance

# Correlation matrix
corr_matrix = df.corr()
plt.figure(figsize=(20,20))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation')
plt.show()

"""**Analyzing if the features follow normal distribution**


*   All the variables observed values close to zero indicating strong evidence that the features don't follow normal distribution


"""

import seaborn as sns
# normality test
from scipy.stats import normaltest

X = df.drop('Class', axis=1)
X_nt = X.drop('Time', axis=1)
y = df[['Class']]

for column in X_nt.columns:
    stat, p = normaltest(X_nt[column])
    print(f'{column}: p={p}')

"""**To Understand the distribution of the variables in the dataset, we use "Box Plots" here**

The main objective here would be to observe and analyze -


*   Potential variablity in data among the given classes
*   Check the inter-quartile ranges for both the classes to see the spread of data
*   To mainly compare distribution of each feature with respect to classes in our dataset





"""

# check distribution of all variables

for column in X_nt.columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='Class', y=column, data=df)
    plt.title(f'Boxplot of {column} by Class')
    plt.show()

# check statistical difference in distributions between 2 classes with mann-whitney
from scipy import stats
from scipy.stats import mannwhitneyu

for column in X_nt.columns:
    class0 = df.loc[df['Class'] == 0, column]
    class1 = df.loc[df['Class'] == 1, column]
    stat, p = stats.ks_2samp(class0, class1)
    print(f'{column}: p={p}, ks_stat:{stat}')

"""**Since we have observed that the features do not follow normal distribution, performing "Mann-Whitney U" tests for each feature to observe statistical differences between the classes.**"""

from scipy import stats
from scipy.stats import mannwhitneyu

#Performing Mann-Whitneyu test for each feature to observe statistical difference between both the classes
for feature in X_nt.columns:
    class0 = df.loc[df['Class'] == 0,feature]
    class1 = df.loc[df['Class'] == 1,feature]
    stat, p = stats.mannwhitneyu(class0, class1)
    print(f'{feature}: p-value={p}')

"""**Feature importance**


* To observe which feature contributes the most to the prediction of a model, we can use tree-based models like Random forest and gradient boosting classifier
* Using feature importance scores from the training model object, we can observe the impact of each feature on the model prediction

Using Random Forest and gradient Boosting Classifiers to get feature importance and getting the average of feature importance scores


"""

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import numpy as np

# Initialize tree-based models
rf_model = RandomForestClassifier()
gb_model = GradientBoostingClassifier()

# Fit the models
rf_model.fit(X, y)
gb_model.fit(X, y)

import matplotlib.pyplot as plt

#Obtain feature importances
rf_feature_importance = rf_model.feature_importances_
gb_feature_importance = gb_model.feature_importances_

#Average of feature importances
average_feature_importance = np.mean([rf_feature_importance, gb_feature_importance], axis=0)

feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'RandomForest_Importance': rf_feature_importance,
    'GradientBoosting_Importance': gb_feature_importance,
    'Average_Importance': average_feature_importance
})

#Sorting by average importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Average_Importance', ascending=False)

#Plotting feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Average_Importance'], color='skyblue')
plt.xlabel('Average Importance')
plt.ylabel('Feature')
plt.title('Average Feature Importance across Models')
plt.gca().invert_yaxis()
plt.show()

"""#**Supervised Learning**


*   Balanced Random Forest Classifier
*   XgBoost
*   Multi-layer Perceptron

#*Balanced Random Forest Classifier*
"""

from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score
from sklearn.model_selection import GridSearchCV

brf_clf = BalancedRandomForestClassifier(random_state=42)

# define the param grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 3, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# grid search
grid_search = GridSearchCV(estimator=brf_clf, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)
y_pred=grid_search.predict(X_test_scaled)

# Best parameters and best score
print("Best parameters:", grid_search.best_params_)
print("Best ROC AUC on train:", grid_search.best_score_)

# Get the probabilistic predictions for the positive class
y_probs = grid_search.predict_proba(X_test_scaled)[:, 1]

# eval
brier_score1 = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score1)
roc_auc1 = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc1)
print()
print('***Classification Report***')
print(classification_report(y_test, y_pred))
report1 = classification_report(y_test, y_pred, output_dict=True)
print()
print('***Confusion Matrix***')
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

"""# XGBoost"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#training-validation-testing split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

#Scaling
scaler = StandardScaler()

#Fit Scaled data
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

import xgboost as xgb

# Calculate the ratio of class 0 to class 1
cntclass_0, cntclass_1 = y_train.value_counts()
scale_pos_weight = cntclass_0 / cntclass_1

# xgbc with adjusted class weight
xgb_clf = xgb.XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight)

# Fit the model with early stopping
eval_set = [(X_val_scaled, y_val)]
xgb_clf.fit(X_train_scaled, y_train, early_stopping_rounds=10, eval_metric="logloss", eval_set=eval_set, verbose=True)
y_pred = xgb_clf.predict(X_test_scaled)

# Get the probabilistic predictions for the positive class
y_probs = xgb_clf.predict_proba(X_test_scaled)[:, 1]

#Evaluation of the training and testing data
#Analyzing brier scores for actual and prediction
brier_score1 = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score1)

#Obtaining ROC AUC score
roc_auc_test = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc_test)

print()
print("#######Classification Report#######")
print(classification_report(y_test, y_pred))
classify_report = classification_report(y_test, y_pred, output_dict=True)

print()
print("#######Confusion Matrix#######")
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

# val set probs
y_valxgbprobs = xgb_clf.predict_proba(X_val_scaled)
y_val_probcl_1 = y_valxgbprobs[:, 1] # class 1 prob

plt.figure(figsize=(10, 6))

# Plot histogram to see class 1 prob distribution
plt.hist(y_val_probcl_1, bins=1000, alpha=0.5, color='skyblue', label='Class 1 Probabilities')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities for Class 1')
plt.legend(loc='best')
plt.grid(True, linestyle='--', alpha=0.5)  # Add grid lines
plt.ylim(0, 250)
plt.xlim(0, 0.05)
plt.show()

# find the threshold with the lowest FN when FP rate doesn't exceed 10%

from sklearn.metrics import confusion_matrix
import numpy as np

thresholds = [0.001, 0.02, 0.03, 0.08, 0.1,0.5,0.8]

best_threshold = None
lowest_FN = np.inf
max_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives

for threshold in thresholds:
    predictions = (y_val_probcl_1 > threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()

    # Check if FP is below 10%
    if fp <= max_allowed_FP:
        # If FP is within the limit, best threshold has the smallest FN
        if fn < lowest_FN:
            lowest_FN = fn
            best_threshold = threshold

print(f"Best Threshold: {best_threshold}")
print(f"Lowest FN (within FP constraint): {lowest_FN}")

# Adjust threshold
y_pred_threshxgb = (y_probs > best_threshold).astype(int)

#Evaluation of the training and testing data
#Analyzing brier scores for actual and prediction
brier_score_bestxgb = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score_bestxgb)

roc_auc_bestxgb = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc_bestxgb)
print()

print("#######Classification Report#######")
print(classification_report(y_test, y_pred_threshxgb))
report2 = classification_report(y_test, y_pred_threshxgb, output_dict=True)

print()
print("#######Confusion Matrix#######")
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_threshxgb, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

"""#Multi_layer Perceptron"""

import tensorflow as tf
tf.config.list_physical_devices()

tf.random.set_seed(42)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df.drop('Class', axis=1)
y = df[['Class']]

# train test val split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

# Scale data

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit scaler
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

import tensorflow.keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.metrics import AUC
from tensorflow.keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt

# MLP model
model = Sequential([
    # 1st layer with 32 neurons
    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    # 2nd layer with 32 neurons
    Dense(32, activation='relu'),
    # add dropout for regularization
    Dropout(0.2),
    # output layer
    Dense(1, activation='sigmoid')
])

# Compile the model (adam lr 0.001 is default)
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', AUC()])

# save model with lowest val loss
model_checkpoint_callback = ModelCheckpoint(
    filepath='best_MLP_model.h5',
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

# train model
epochs_hist = model.fit(X_train_scaled, y_train, epochs=30, batch_size=20, verbose=1, validation_data=(X_val_scaled, y_val), callbacks=[model_checkpoint_callback])

# Plot the train and validation loss
plt.plot(epochs_hist.history['loss']) # Training loss
plt.plot(epochs_hist.history['val_loss']) # Validation loss
plt.title('Model Loss Onserved During Training/Validation')
plt.ylabel('Training and Validation Losses')
plt.xlabel('Epochs')
plt.legend(['Training Loss','Validation Loss'])

# Plot the train and validation loss
plt.plot(epochs_hist.history['loss']) # Training loss
plt.plot(epochs_hist.history['val_loss']) # Validation loss
plt.title('Model Loss Observed During Training/Validation')
plt.ylabel('Training and Validation Losses')
plt.xlabel('Epochs')
plt.legend(['Training Loss','Validation Loss'])

from tensorflow.keras.models import load_model

# Load the saved model
mlp_model = load_model('best_MLP_model.h5')

# Eval test set
test_loss, test_acc, test_auc = mlp_model.evaluate(X_test_scaled, y_test, verbose=1)

validation_probs = model.predict(X_val_scaled)

# Determine optimal threshold based on val prob distribution

plt.figure(figsize=(10, 6))

# Plot histogram to see class 1 prob distribution
plt.hist(validation_probs, bins=500, alpha=0.5, label='Class 1 Probabilities')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of distribution of Predicted Probabilities for Class 0 and Class 1')
plt.legend(loc='best')
plt.ylim(0, 60)
plt.xlim(0, 1)
plt.show()

import numpy as np

# find the threshold with the lowest FN when FP rate doesn't exceed 10%

from sklearn.metrics import confusion_matrix
thresholds = [0.0001,0.0002, 0.005, 0.0009, 0.001, 0.002, 0.004,0.008,0.01]

best_threshold = None
lowest_FN = np.inf
max_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives

for threshold in thresholds:
    predictions = (validation_probs > threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()

    # Check if FP is below 10%
    if fp <= max_allowed_FP:
        # If FP is within the limit, best threshold has the smallest FN
        if fn < lowest_FN:
            lowest_FN = fn
            best_threshold = threshold

print(f"Best Threshold: {best_threshold}")
print(f"Lowest FN (within FP constraint): {lowest_FN}")

from sklearn.metrics import roc_curve, precision_recall_curve, f1_score
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score
from sklearn.model_selection import GridSearchCV
import seaborn as sns
import numpy as np

# get probabilites
y_probs = model.predict(X_test_scaled).ravel()

# Adjust threshold
y_pred_threshmlp = (y_probs > best_threshold).astype(int)


#Evaluation of the training and testing data
#Analyzing brier scores for actual and prediction
brier_score_bestmlp = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score_bestmlp)

roc_auc_bestmlp = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc_bestmlp)
print()

print("#######Classification Report#######")
print(classification_report(y_test, y_pred_threshmlp))
report2 = classification_report(y_test, y_pred_threshmlp, output_dict=True)

print()
print("#######Confusion Matrix#######")
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_threshmlp, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

"""#Unsupervised Learning


* PCA
* Autoencoders

# PCA
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# train test split
X = df.drop('Class', axis=1)
y = df[['Class']]

# train test val split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Scaling
scaler = StandardScaler()

#Fit scaled data
scaler.fit(X_train)

X_train_sc = scaler.transform(X_train)
X_test_sc= scaler.transform(X_test)

from sklearn.decomposition import PCA

#PCA for training
pca_train = PCA(.95)
pca_train.fit(X_train_sc)
X_pca_train= pca_train.transform(X_train_sc)
X_train_afterpca = pca_train.inverse_transform(X_pca_train)

#PCA for testing
pca_test = PCA(.95)
pca_test.fit(X_test_sc)
X_pca_test=pca_test.transform(X_test_sc)
X_test_afterpca = pca_test.inverse_transform(X_pca_test)

import numpy as np

# Reconstruction error for training data
reconstruction_errors_train = np.mean(np.square(X_train_sc - X_train_afterpca), axis=1)

# Reconstruction error for testing data
reconstruction_errors_test = np.mean(np.square(X_test_sc - X_test_afterpca), axis=1)

print("Array of reconstruction errors for training data:", reconstruction_errors_train)
print("Array of reconstruction errors for testing data:", reconstruction_errors_test)

np.max(reconstruction_errors_train)

import numpy as np
import matplotlib.pyplot as plt

# Calculate bin widths for x-axis
bin_width_train = (max(reconstruction_errors_train) - min(reconstruction_errors_train)) / 20
bin_width_test = (max(reconstruction_errors_test) - min(reconstruction_errors_test)) / 20

# Plot histograms of reconstruction errors
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.hist(reconstruction_errors_train, bins=np.arange(min(reconstruction_errors_train), max(reconstruction_errors_train) + bin_width_train, bin_width_train), color='blue', alpha=0.7)
plt.title('Histogram of Reconstruction Errors (Training)')
plt.xlabel('Reconstruction Error')
plt.ylabel('Frequency')
plt.ylim(0, max(len(np.histogram(reconstruction_errors_train, bins='auto')[0]), len(np.histogram(reconstruction_errors_test, bins='auto')[0])))

plt.subplot(1, 2, 2)
plt.hist(reconstruction_errors_test, bins=np.arange(min(reconstruction_errors_test), max(reconstruction_errors_test) + bin_width_test, bin_width_test), color='red', alpha=0.7)
plt.title('Histogram of Reconstruction Errors (Testing)')
plt.xlabel('Reconstruction Error')
plt.ylabel('Frequency')
plt.ylim(0, max(len(np.histogram(reconstruction_errors_train, bins='auto')[0]), len(np.histogram(reconstruction_errors_test, bins='auto')[0])))

plt.tight_layout()
plt.show()

# Calculate the 99th percentile threshold
threshold_train = np.percentile(reconstruction_errors_train, 99)
threshold_test = np.percentile(reconstruction_errors_test, 99)

# Identify outliers
outliers_train = reconstruction_errors_train[reconstruction_errors_train > threshold_train]
outliers_test = reconstruction_errors_test[reconstruction_errors_test > threshold_test]

print("99th percentile threshold for training data:", threshold_train)
print("Number of outliers in training data:", len(outliers_train))
print("99th percentile threshold for testing data:", threshold_test)
print("Number of outliers in testing data:", len(outliers_test))

from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_train.values.ravel(), reconstruction_errors_train, pos_label=1)

# Find the threshold with lowest FN when FP rate doesn't exceed 10%
best_threshold = None
lowest_FN = np.inf
max_allowed_FP = 0.1 * np.sum(y_train.values.ravel() == 0)  # 10% of all true negatives

for threshold in thresholds:
    # Count false positives and false negatives
    fp = np.sum((reconstruction_errors_train > threshold) & (y_train.values.ravel() == 0))
    fn = np.sum((reconstruction_errors_train <= threshold) & (y_train.values.ravel() == 1))

    # Check if FP is below 10%
    if fp <= max_allowed_FP:
        # If FP is within the limit, update best threshold if FN is lower
        if fn < lowest_FN:
            lowest_FN = fn
            best_threshold = threshold

print(f"Best Threshold: {best_threshold}")
print(f"Lowest FN (within FP constraint): {lowest_FN}")

from sklearn.metrics import roc_curve, precision_recall_curve, f1_score
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score
from sklearn.model_selection import GridSearchCV
import seaborn as sns
import numpy as np

# Use the best threshold on the test set
test_predictions = np.where(reconstruction_errors_test > best_threshold, 1, 0)

# Evaluate
print('####Classification Report####')
print(classification_report(y_test.values.ravel(), test_predictions))
report3 = classification_report(y_test.values.ravel(), test_predictions, output_dict=True)
print()
print('####Confusion Matrix####')
conf_matrix = pd.crosstab(y_test.values.ravel(), test_predictions, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

"""Still Too many FN

#AutoEncoder
"""

X = df.drop('Class', axis=1)
y = df[['Class']]

# training-testing-validation split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

#scaling
scaler = StandardScaler()

#Scaling the data
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

from keras.layers import Input, Dense
from keras.models import Model

#Defining Autoencoder
custom_input_layer = Input(shape=(X_train.shape[1],))
custom_encoder = Dense(32, activation="relu")(custom_input_layer)
custom_decoder = Dense(X_train.shape[1], activation="sigmoid")(custom_encoder)

# Initialize model
custom_autoencoder = Model(custom_input_layer, custom_decoder)
custom_autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder
custom_autoencoder.fit(X_train_scaled,X_train_scaled,
                       epochs=50,
                       batch_size=256,
                       shuffle=True,
                       validation_data=(X_val_scaled, X_val_scaled))

# Calculate reconstruction error for validation
custom_reconstructed = custom_autoencoder.predict(X_val_scaled)
custom_mse_val = np.mean(np.power(X_val_scaled - custom_reconstructed, 2), axis=1)

import matplotlib.pyplot as plt

# Plot histogram of custom MSE for validation
plt.figure(figsize=(8, 5))
plt.hist(custom_mse_val, bins=range(0, 51), color='blue', alpha=0.7)
plt.title('Histogram of Custom MSE for Validation')
plt.xlabel('MSE')
plt.ylabel('Frequency')
plt.xlim(0, 50)  # Set x-axis range from 0 to 50
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr_custom, tpr_custom, thresholds_custom = roc_curve(y_val.values.ravel(), custom_mse_val, pos_label=1)

# Find the threshold with lowest FN when FP rate doesn't exceed 10%
best_threshold_custom = None
lowest_FN_custom = np.inf
max_allowed_FP_custom = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives

# Iterate over thresholds to find the best one
for i, threshold_custom in enumerate(thresholds_custom):
    # Compute false positive and false negative counts
    fp_custom = np.sum((custom_mse_val > threshold_custom) & (y_val.values.ravel() == 0))
    fn_custom = np.sum((custom_mse_val <= threshold_custom) & (y_val.values.ravel() == 1))

    # Check if false positive rate is below 10%
    if fp_custom <= max_allowed_FP_custom:
        # If FP rate is within the limit, update best threshold if FN is lower
        if fn_custom < lowest_FN_custom:
            lowest_FN_custom = fn_custom
            best_threshold_custom = threshold_custom

# Output the results
print(f"Best Threshold: {best_threshold_custom}")
print(f"Lowest FN (within FP constraint): {lowest_FN_custom}")

# calculate reconstruction error for test
y_predicted_autoenc = custom_autoencoder.predict(X_test_scaled)
mse_test_pred_autoenc = np.mean(np.power(X_test_scaled - y_predicted_autoenc, 2), axis=1)

# use this threshold on the test set

test_predictions = np.where(mse_test_pred_autoenc > best_threshold_custom, 1, 0)

# eval
print('***Classification Report***')
print(classification_report(y_test.values.ravel(), test_predictions))
report3 = classification_report(y_test.values.ravel(), test_predictions, output_dict=True)
print()
print('***Confusion Matrix***')
conf_matrix = pd.crosstab(y_test.values.ravel(), test_predictions, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

"""#Xgboost- Feature Selection

As observed from statistical difference Mann-Whitney U test, "V13", "V15" and "V22" are the features which do not show statistical difference between classes. So, we can drop them and try fitting the model to achieve minimal False Negatives
"""

# drop 'V13','V15','V22' since they are not statisitcally significantly different across classes
X = df.drop(['Class','V13','V15','V22'], axis=1)
y = df[['Class']]

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# train test val split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

# Scale data

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit scaler
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Calculate the ratio of class 0 to class 1
count_class_0, count_class_1 = y_train.value_counts()
scale_pos_weight = count_class_0 / count_class_1

# xgbc with adjusted class weight
xgbc = xgb.XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight)

# Fit the model with early stopping
eval_set = [(X_val_scaled, y_val)]
xgbc.fit(X_train_scaled, y_train, early_stopping_rounds=10, eval_metric="logloss", eval_set=eval_set, verbose=True)
y_pred = xgbc.predict(X_test_scaled)

# Get the probabilistic predictions for the positive class
y_probs = xgbc.predict_proba(X_test_scaled)[:, 1]

from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score
import seaborn as sns

# eval
brier_score2 = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score2)
roc_auc2 = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc2)
print()
print('***Classification Report***')
print(classification_report(y_test, y_pred))
report2 = classification_report(y_test, y_pred, output_dict=True)
print()
print('***Confusion Matrix***')
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))

import matplotlib.pyplot as plt
probs = xgbc.predict_proba(X_val_scaled) # val set probs
probs_class_1 = probs[:, 1] # class 1 prob

plt.figure(figsize=(10, 6))

# Plot histogram to see class 1 prob distribution
plt.hist(probs_class_1, bins=1000, alpha=0.5, label='Class 1 Probabilities')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities for Class 0 and Class 1')
plt.legend(loc='best')
plt.ylim(0, 500)
plt.xlim(0, 0.05)
plt.show()

# find the threshold with the lowest FN when FP rate doesn't exceed 10%

from sklearn.metrics import confusion_matrix
import numpy as np

thresholds = [0.001, 0.02, 0.03, 0.08, 0.1,0.5,0.8]

best_threshold = None
lowest_FN = np.inf
max_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives

for threshold in thresholds:
    predictions = (probs_class_1 > threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()

    # Check if FP is below 10%
    if fp <= max_allowed_FP:
        # If FP is within the limit, best threshold has the smallest FN
        if fn < lowest_FN:
            lowest_FN = fn
            best_threshold = threshold

print(f"Best Threshold: {best_threshold}")
print(f"Lowest FN (within FP constraint): {lowest_FN}")

from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_val.values.ravel(), probs_class_1)

# Find the threshold with lowest FN when FP rate doesn't exceed 10%
best_threshold = None
lowest_FN = np.inf
max_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives

# Iterate over thresholds to find the best one
for i, threshold in enumerate(thresholds):
    # Compute false positive and false negative counts
    predictions = (probs_class_1 > threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()

    # Check if false positive rate is below 10%
    if fp <= max_allowed_FP:
        # If FP rate is within the limit, update best threshold if FN is lower
        if fn < lowest_FN:
            lowest_FN = fn
            best_threshold = threshold

# Output the results
print(f"Best Threshold: {best_threshold}")
print(f"Lowest FN (within FP constraint): {lowest_FN}")

# Adjust threshold
y_pred_adj = (y_probs > best_threshold).astype(int)

from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score
from sklearn.model_selection import GridSearchCV
import seaborn as sns

brier_score2 = brier_score_loss(y_test, y_probs)
print("Brier score:", brier_score2)
roc_auc2 = roc_auc_score(y_test, y_probs)
print("ROC AUC on test:", roc_auc2)
print()
print('***Classification Report***')
print(classification_report(y_test, y_pred_adj))
report2 = classification_report(y_test, y_pred_adj, output_dict=True)
print()
print('***Confusion Matrix***')
conf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_adj, rownames=['Actual'], colnames=['Predicted'])
print(sns.heatmap(conf_matrix, annot=True, fmt="d"))